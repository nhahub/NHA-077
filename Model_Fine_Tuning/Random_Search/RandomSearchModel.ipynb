{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fc475c3",
   "metadata": {},
   "source": [
    "# Land Type Classification using Sentinel-2 Satellite Images\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c691ebd",
   "metadata": {},
   "source": [
    "#### Some tests for GPU `tensorflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e212b070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "772ed0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: 64_112\n",
      "cuDNN: 64_8\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.platform.build_info import build_info\n",
    "\n",
    "print(\"CUDA:\", build_info[\"cuda_version\"])\n",
    "print(\"cuDNN:\", build_info[\"cudnn_version\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bcf837",
   "metadata": {},
   "source": [
    "#### Limited VRAM usage as CUDA allocate the whole entirety of VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3d7a13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[/] Limited GPU memory to 2 GB.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.config.experimental import list_physical_devices, VirtualDeviceConfiguration, set_virtual_device_configuration #type:ignore\n",
    "vram_GB = 2 \n",
    "gpus = list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        set_virtual_device_configuration( gpus[0], [VirtualDeviceConfiguration(memory_limit= vram_GB * 1024)] )\n",
    "        print(f\"[/] Limited GPU memory to {vram_GB} GB.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709072c2",
   "metadata": {},
   "source": [
    "### Had to re-define `CategoricalFocalLoss` as it was missing in this old version of tensorflow\n",
    " - should have the same functionanlity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b1cb73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import Loss #type:ignore\n",
    "from tensorflow.keras import backend as K #type:ignore\n",
    "\n",
    "class CategoricalFocalLoss(Loss):\n",
    "    \"\"\"\n",
    "    Categorical focal loss for one-hot labels.\n",
    "    Usage:\n",
    "        loss = CategoricalFocalLoss(gamma=2.0, alpha=0.25)\n",
    "        model.compile(..., loss=loss, ...)\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=2.0, alpha=0.25, from_logits=False, reduction=tf.keras.losses.Reduction.AUTO, name=\"categorical_focal_loss\"):\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "        self.gamma = float(gamma)\n",
    "        self.alpha = float(alpha) if alpha is not None else None\n",
    "        self.from_logits = from_logits\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        y_true: one-hot (batch, classes)\n",
    "        y_pred: probabilities (softmax) or logits if from_logits=True\n",
    "        \"\"\"\n",
    "        # If logits provided, convert to probabilities\n",
    "        if self.from_logits:\n",
    "            y_pred = tf.nn.softmax(y_pred, axis=-1)\n",
    "\n",
    "        # Clip to avoid log(0) and NaNs\n",
    "        eps = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, eps, 1.0 - eps)\n",
    "\n",
    "        # Compute cross-entropy per class\n",
    "        cross_entropy = -y_true * K.log(y_pred)  # shape (batch, classes)\n",
    "\n",
    "        # get p_t: the model probability for the true class per sample (elementwise y_true * y_pred)\n",
    "        p_t = y_true * y_pred  # (batch, classes)\n",
    "\n",
    "        # focal weighting: (1 - p_t)^gamma\n",
    "        focal_factor = K.pow(1.0 - p_t, self.gamma)\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha_factor = y_true * self.alpha + (1.0 - y_true) * (1.0 - self.alpha)\n",
    "            # For multiclass one-hot alpha factor reduces to alpha for true class; keep simple:\n",
    "            alpha_factor = y_true * self.alpha  # only apply alpha to true class entries\n",
    "        else:\n",
    "            alpha_factor = 1.0\n",
    "\n",
    "        # elementwise focal loss\n",
    "        loss = alpha_factor * focal_factor * cross_entropy  # (batch, classes)\n",
    "\n",
    "        # sum over classes then mean over batch\n",
    "        loss = K.sum(loss, axis=-1)\n",
    "        return K.mean(loss)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"gamma\": self.gamma, \"alpha\": self.alpha, \"from_logits\": self.from_logits}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330a8441",
   "metadata": {},
   "source": [
    "***\n",
    "# Normal Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c024c9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "from tensorflow.keras.regularizers import l2 #type:ignore\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD #type:ignore\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential #type:ignore\n",
    "from tensorflow.keras.applications import EfficientNetB3 #type:ignore\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "#from tensorflow.keras.losses import CategoricalFocalCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint #type:ignore\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator #type:ignore\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout #type:ignore\n",
    "from tensorflow.keras.layers import BatchNormalization, GlobalAveragePooling2D, LeakyReLU, AveragePooling2D #type:ignore\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef1e6fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9cbe01",
   "metadata": {},
   "source": [
    "#### Split data into folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47010076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root:        d:\\Coding\\Jupyter\\DEPI\\DEPI-Final-Project\n",
      "Dataset Root:        d:\\Coding\\Jupyter\\DEPI\\DEPI-Final-Project\\Dataset\n",
      "Original Root:       d:\\Coding\\Jupyter\\DEPI\\DEPI-Final-Project\\Dataset\\EuroSAT_RGB\n",
      "Base Folder:         d:\\Coding\\Jupyter\\DEPI\\DEPI-Final-Project\\Dataset\\EuroSAT_RGB_split\n",
      "Checkpoints Folder:  d:\\Coding\\Jupyter\\DEPI\\DEPI-Final-Project\\Model_Fine_Tuning\\Random_Search\\Checkpoints\n",
      "Results File:        d:\\Coding\\Jupyter\\DEPI\\DEPI-Final-Project\\Model_Fine_Tuning\\Random_Search\\random_search_results.csv\n"
     ]
    }
   ],
   "source": [
    "project_root = Path(os.getcwd()).parent.parent\n",
    "    \n",
    "dataset_root = project_root / \"Dataset\"\n",
    "original_folder = dataset_root / \"EuroSAT_RGB\"\n",
    "base_folder = dataset_root / \"EuroSAT_RGB_split\"\n",
    "\n",
    "checkpoints_dir = project_root / \"Model_Fine_Tuning\" / \"Random_Search\" / \"Checkpoints\"\n",
    "results_file = project_root / \"Model_Fine_Tuning\" / \"Random_Search\" / \"random_search_results.csv\"\n",
    "\n",
    "os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "\n",
    "print(\"Project Root:       \",project_root)\n",
    "print(\"Dataset Root:       \",dataset_root)\n",
    "print(\"Original Root:      \",original_folder)\n",
    "print(\"Base Folder:        \",base_folder)\n",
    "print(\"Checkpoints Folder: \",checkpoints_dir)\n",
    "print(\"Results File:       \",results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2da9595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data():\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    random_state = 42\n",
    "    random.seed(random_state)\n",
    "\n",
    "    for split in [\"train\", \"validate\", \"test\"]:\n",
    "        os.makedirs(os.path.join(base_folder, split), exist_ok=True)\n",
    "    \n",
    "    for class_name in os.listdir(original_folder):\n",
    "        class_path = os.path.join(original_folder, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        images = [f for f in os.listdir(class_path) if f.lower().endswith((\".jpg\", \".png\", \".tif\"))]\n",
    "        random.shuffle(images)\n",
    "\n",
    "        n_total = len(images)\n",
    "        n_train = int(n_total * train_ratio)\n",
    "        n_val = int(n_total * val_ratio)\n",
    "\n",
    "        splits = {\n",
    "            \"train\": images[:n_train],\n",
    "            \"validate\": images[n_train:n_train + n_val],\n",
    "            \"test\": images[n_train + n_val:]\n",
    "        }\n",
    "\n",
    "        for split_name, split_images in splits.items():\n",
    "            split_dir = os.path.join(base_folder, split_name, class_name)\n",
    "            os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "            for img_name in split_images:\n",
    "                src = os.path.join(class_path, img_name)\n",
    "                dst = os.path.join(split_dir, img_name)\n",
    "                shutil.copy(src, dst)\n",
    "\n",
    "        print(f\"{class_name}: {n_total} images split into train/val/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5d11adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9550d0",
   "metadata": {},
   "source": [
    "#### split_data() Output:\n",
    " - AnnualCrop: 3000 images split into train/val/test\n",
    " - Forest: 3000 images split into train/val/test\n",
    " - HerbaceousVegetation: 3000 images split into train/val/test\n",
    " - Highway: 2500 images split into train/val/test\n",
    " - Industrial: 2500 images split into train/val/test\n",
    " - Pasture: 2000 images split into train/val/test\n",
    " - PermanentCrop: 2500 images split into train/val/test\n",
    " - Residential: 3000 images split into train/val/test \n",
    " - River: 2500 images split into train/val/test \n",
    " - SeaLake: 3000 images split into train/val/test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fe5aeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = base_folder / \"train\"\n",
    "val_dir = base_folder / \"validate\"\n",
    "test_dir = base_folder / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d838116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18900 images belonging to 10 classes.\n",
      "Found 4050 images belonging to 10 classes.\n",
      "Found 4050 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False \n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a36acfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from trial 161...\n"
     ]
    }
   ],
   "source": [
    "completed_trials = 0\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'r') as f:\n",
    "        completed_trials = sum(1 for _ in f) - 1\n",
    "print(f\"Resuming from trial {completed_trials + 1}...\")\n",
    "\n",
    "if not os.path.exists(results_file):\n",
    "    with open(results_file, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['trial', 'filters', 'conv_layers', 'dense_units', 'dropout', 'batch_norm', 'optimizer', 'lr', 'accuracy', 'precision', 'recall', 'f1_score', 'weighted_score','flattening_layers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "334711e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.9,\n",
       " 1: 0.9,\n",
       " 2: 0.9,\n",
       " 3: 1.08,\n",
       " 4: 1.08,\n",
       " 5: 1.35,\n",
       " 6: 1.08,\n",
       " 7: 0.9,\n",
       " 8: 1.08,\n",
       " 9: 0.9}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels = train_generator.classes\n",
    "class_weight_dict = dict(\n",
    "    enumerate(\n",
    "        compute_class_weight('balanced', classes=np.unique(class_labels), y=class_labels)\n",
    "    )\n",
    ")\n",
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2444086",
   "metadata": {},
   "source": [
    "***\n",
    "### **Model (**`CNN`**) Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901b54ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "num_trials = 160\n",
    "        \n",
    "for k in range(completed_trials, num_trials):\n",
    "    print(f\"\\n=== Trial {k+1}/{num_trials} ===\")\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    callbacks = [\n",
    "        ModelCheckpoint(str(checkpoints_dir / f'best_model_{k}.h5'), monitor='val_loss', save_best_only=True, verbose=0),\n",
    "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, min_lr=1e-6)\n",
    "    ]\n",
    "    \n",
    "    params = { \n",
    "        'filters': random.choice([32, 64]), # 64\n",
    "        'conv_layers': random.choice([2, 3, 4]), # 4\n",
    "        'dense_units': random.choice([128, 256, 512]), # 256\n",
    "        'dropout': round(random.uniform(0.3, 0.6), 2), # 0.4\n",
    "        'batch_norm': random.choice([True, False]), # True\n",
    "        'optimizer': random.choice(['adam', 'rmsprop', 'sgd']), # adam\n",
    "        'lr': random.choice([1e-4, 5e-4, 1e-3, 5e-3]), # 1e-4\n",
    "        'flattening_layers': random.choice([\"flatten\", \"GAP2D\"])\n",
    "    }\n",
    "\n",
    "    model = Sequential()\n",
    "    filters = params['filters']\n",
    "\n",
    "    for i in range(params['conv_layers']):\n",
    "        if i == 0:\n",
    "            model.add(Conv2D(filters, (3,3), activation='relu', input_shape=(64, 64, 3)))\n",
    "        else:\n",
    "            model.add(Conv2D(filters, (3,3), activation='relu'))\n",
    "        model.add(MaxPooling2D(2,2))\n",
    "        if params['batch_norm']:\n",
    "            model.add(BatchNormalization())\n",
    "        filters *= 2  # Double filters each block\n",
    "        \n",
    "    if params[\"flattening_layers\"] == \"GAP2D\":\n",
    "        model.add(GlobalAveragePooling2D())\n",
    "        model.add(Dense(params['dense_units'], activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    else:\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(params['dense_units'], activation='relu'))\n",
    "        \n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
    "\n",
    "    if params['optimizer'] == 'adam':\n",
    "        opt = Adam(learning_rate=params['lr'])\n",
    "    elif params['optimizer'] == 'rmsprop':\n",
    "        opt = RMSprop(learning_rate=params['lr'])\n",
    "    else:\n",
    "        opt = SGD(learning_rate=params['lr'], momentum=0.9)\n",
    "\n",
    "    model.compile(optimizer=opt, loss=CategoricalFocalLoss(gamma=2.0, alpha=0.25, from_logits=False), metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        epochs=50,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    val_generator.reset()\n",
    "    preds = model.predict(val_generator, verbose=0)\n",
    "    y_true = val_generator.classes\n",
    "    y_pred = np.argmax(preds, axis=1)\n",
    "\n",
    "    acc = np.mean(y_true == y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    weighted_score = acc * precision * recall * f1\n",
    "\n",
    "    with open(results_file, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            k + 1,\n",
    "            params['filters'],\n",
    "            params['conv_layers'],\n",
    "            params['dense_units'],\n",
    "            params['dropout'],\n",
    "            params['batch_norm'],\n",
    "            params['optimizer'],\n",
    "            params['lr'],\n",
    "            round(acc, 6),\n",
    "            round(precision, 6), #type:ignore\n",
    "            round(recall, 6), #type:ignore\n",
    "            round(f1, 6), #type:ignore\n",
    "            round(weighted_score, 6),\n",
    "            params['flattening_layers']\n",
    "        ])\n",
    "\n",
    "    print(f\"[/] Saved trial {k+1}: acc={acc:.4f}, f1={f1:.4f}, weighted={weighted_score:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa7f393",
   "metadata": {},
   "source": [
    "***\n",
    "#### print used VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5522d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: /physical_device:GPU:0\n",
      "  Used VRAM: 0.00 MB\n",
      "  Peak VRAM: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "details = tf.config.experimental.get_memory_info(gpus[0].name[17:])\n",
    "print(f\"GPU: {gpus[0].name}\")\n",
    "print(f\"  Used VRAM: {details['current'] / 1024**2:.2f} MB\")\n",
    "print(f\"  Peak VRAM: {details['peak'] / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6341284",
   "metadata": {},
   "source": [
    "#### **Model Test**\n",
    "**Returns:**\n",
    " - Test Accuracy\n",
    " - Test Loss\n",
    " - y Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5d81d7",
   "metadata": {},
   "source": [
    "***\n",
    "#### Read `random_search_results.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a88cdee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>filters</th>\n",
       "      <th>conv_layers</th>\n",
       "      <th>dense_units</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batch_norm</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lr</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>weighted_score</th>\n",
       "      <th>flattening_layers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>147</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>0.32</td>\n",
       "      <td>True</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.960247</td>\n",
       "      <td>0.960594</td>\n",
       "      <td>0.960247</td>\n",
       "      <td>0.960196</td>\n",
       "      <td>0.850483</td>\n",
       "      <td>GAP2D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>0.46</td>\n",
       "      <td>True</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.952840</td>\n",
       "      <td>0.952929</td>\n",
       "      <td>0.952840</td>\n",
       "      <td>0.952639</td>\n",
       "      <td>0.824192</td>\n",
       "      <td>GAP2D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>126</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>0.33</td>\n",
       "      <td>True</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.945926</td>\n",
       "      <td>0.946351</td>\n",
       "      <td>0.945926</td>\n",
       "      <td>0.945805</td>\n",
       "      <td>0.800881</td>\n",
       "      <td>GAP2D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>142</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>0.59</td>\n",
       "      <td>True</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.944842</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.944300</td>\n",
       "      <td>0.795833</td>\n",
       "      <td>GAP2D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>0.56</td>\n",
       "      <td>True</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.943951</td>\n",
       "      <td>0.945732</td>\n",
       "      <td>0.943951</td>\n",
       "      <td>0.943971</td>\n",
       "      <td>0.795472</td>\n",
       "      <td>GAP2D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>512</td>\n",
       "      <td>0.52</td>\n",
       "      <td>True</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.217284</td>\n",
       "      <td>0.066192</td>\n",
       "      <td>0.217284</td>\n",
       "      <td>0.099410</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>flatten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>33</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.52</td>\n",
       "      <td>True</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.185679</td>\n",
       "      <td>0.055198</td>\n",
       "      <td>0.185679</td>\n",
       "      <td>0.080039</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>flatten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>24</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>0.53</td>\n",
       "      <td>True</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>flatten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>76</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>0.54</td>\n",
       "      <td>False</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>flatten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>69</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>0.41</td>\n",
       "      <td>False</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>flatten</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     trial  filters  conv_layers  dense_units  dropout  batch_norm optimizer  \\\n",
       "0      147       64            3          128     0.32        True      adam   \n",
       "1      137       64            3          128     0.46        True   rmsprop   \n",
       "2      126       32            3          128     0.33        True      adam   \n",
       "3      142       64            3          256     0.59        True   rmsprop   \n",
       "4       79       64            3          256     0.56        True   rmsprop   \n",
       "..     ...      ...          ...          ...      ...         ...       ...   \n",
       "155     50       32            3          512     0.52        True      adam   \n",
       "156     33       32            2          256     0.52        True      adam   \n",
       "157     24       32            3          256     0.53        True      adam   \n",
       "158     76       64            2          512     0.54       False   rmsprop   \n",
       "159     69       32            4          128     0.41       False      adam   \n",
       "\n",
       "         lr  accuracy  precision    recall  f1_score  weighted_score  \\\n",
       "0    0.0010  0.960247   0.960594  0.960247  0.960196        0.850483   \n",
       "1    0.0005  0.952840   0.952929  0.952840  0.952639        0.824192   \n",
       "2    0.0010  0.945926   0.946351  0.945926  0.945805        0.800881   \n",
       "3    0.0001  0.944444   0.944842  0.944444  0.944300        0.795833   \n",
       "4    0.0005  0.943951   0.945732  0.943951  0.943971        0.795472   \n",
       "..      ...       ...        ...       ...       ...             ...   \n",
       "155  0.0050  0.217284   0.066192  0.217284  0.099410        0.000311   \n",
       "156  0.0050  0.185679   0.055198  0.185679  0.080039        0.000152   \n",
       "157  0.0050  0.111111   0.012346  0.111111  0.022222        0.000003   \n",
       "158  0.0050  0.111111   0.012346  0.111111  0.022222        0.000003   \n",
       "159  0.0050  0.111111   0.012346  0.111111  0.022222        0.000003   \n",
       "\n",
       "    flattening_layers  \n",
       "0               GAP2D  \n",
       "1               GAP2D  \n",
       "2               GAP2D  \n",
       "3               GAP2D  \n",
       "4               GAP2D  \n",
       "..                ...  \n",
       "155           flatten  \n",
       "156           flatten  \n",
       "157           flatten  \n",
       "158           flatten  \n",
       "159           flatten  \n",
       "\n",
       "[160 rows x 14 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(results_file)\n",
    "df_sorted = df.sort_values(by='weighted_score', ascending=False)\n",
    "df_sorted.reset_index(drop=True, inplace=True)\n",
    "df_sorted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
